# OpenAI Deep Research System Card

February 25, 2025

*本身工作流程是一点没谈，光说这个系统多好多好了，风险小了，可以为pro用户好好用了。不过一些评估手段还是可以借鉴的*

### 1 Introduction

定义：一种新的代理式能力，可在互联网上进行多步骤研究，适用于复杂任务

功能：搜索、解析、分析大量文本、图像和PDF，能编写并执行Python代码



### 2 Model data and training

使用专门构建的新型网页浏览数据集，其中有两类：有标准答案与开放式任务。

训练方式为通过强化学习训练进行网页浏览、使用Python工具等技能的学习。



### 3 Risk identification, assessment and mitigation

外部红队攻击：可能存在个人隐私泄漏、非法内容生成，可加强隐私保护、训练模型抵抗恶意指令用于缓解。

评估方法：deep research可以利用综合知识提出新见解，较难评估内容好坏。所以在此只对其生成内容的安全评估



##### **主要风险**及其缓解措施

提示注入攻击（Prompt Injection）：模型浏览的网页中包含恶意指令，诱导非法动作
新增安全训练数据、系统级缓解措施（不允许深度研究导航到或构建任意URL）

禁止内容生成（Disallowed Content）：可能生成促进暴力、危险活动的详细指南，或提供敏感建议
更新安全策略与训练数据、加强对敏感请求的拒绝能力。

隐私泄露风险（Privacy）：从多个公开来源聚合个人信息形成对个体的“全景画像”
更新政策、新增安全训练数据与评估集、监控

代码执行风险（Code Execution）：使用python工具可能发起网络攻击或数据窃取。
Python 工具无网络访问权限，或在沙盒中进行。

偏见（Bias）：过度依赖网络搜索可能导致模型继承并放大网络中的偏见。
后训练阶段奖励减少偏见的拒绝行为。

幻觉（Hallucination）：模型可能生成不实信息。
强化对事实性的奖励机制、利用网络搜索作为事实依据，减少虚构。



**准备度框架评估**（Preparedness Framework Evaluations）：识别前沿模型可能带来的灾难性风险。标准为只有事后缓解评分为**“中等”或以下**的模型才能部署，只有事后缓解评分为“高”或以下的模型才能进一步开发。其评估旨在测试代表“**已知最坏情况**”的事前缓解风险模型。
需要先说明的是，由于deeep research会从网页中汲取知识，所以其可能直接找到评估的答案，所以openai的一些前沿评估保证不会受到污染，因为数据从未发布；其加强了缓解措施

结果表明，该deep research在网络安全（Cybersecurity）、CBRN（化生放核）、劝服能力（Persuasion）、模型自主性（Model Autonomy）方面**整体风险等级为 Medium**。



























