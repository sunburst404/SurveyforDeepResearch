# DEEP RESEARCH AGENTS: 

# A SYSTEMATIC EXAMINATION AND ROADMAP

*感觉框架比较清晰，没太多废话，技术实现的细节我感觉还是不够*

### 1 Introduction

定义deep research agents：由LLMs驱动的AI代理，集成动态推理、适应性规划和迭代工具使用，以获取、聚合和分析**外部信息**，最终为完成开放式信息研究任务提供**全面**输出。

这种架构使得DR代理能够通过将推理过程与**多模态资源**无缝集成，自主管理复杂、端到端的研究工作流。

![image-20250909201304743](E:\DailyWork\25-8\images\image-20250909201304743.png)

相较于RAG，DR代理能够独特地处理复杂、 演变和知识密集型的研究场景。



### 2 Background and Preliminaries

#### 2.1 Advances in Reasoning and Tool Integration

在CoT（可解释性、准确性）的基础 上，后续研究引入了增强LLM推理的方法，特别是在处理长文本上下文方面。
为应对需要实时或专业外部知识的推理任务，一些框架使大语言模型能够自主整合外部计算资源并直接在推理工作流中调用 API。

局限：幻觉、静态或过时的内部知识，以及对快速变化的信息需求响应不足

#### 2.2 Advances in Retrieval-Augmented Generation and Agentic Retrieval

检索增强生成 (RAG)：利用外部知识库（例如，网络、API），已成为一种有效策略，以减轻幻觉问题并提 高网络信息搜索的准确性；但处理多步复杂能力有限。

代理RAG：利用迭代检索、自适应查询和动态工作流调整，显著增 强了多步推理能力；动态开销、用户一致性存在限制、语料库实时性不够
解决这一挑战需要将外部API和网络浏览能力集 成到RAG架构中，从而推动最近DR方法（实时的检索并分析）的发展，旨在进一步提高检索全面性和适应性。

#### 2.3 Model Context Protocol and Agent-to-Agent Policy

MCP（模型上下文协议）：一个统一的通信层，允许基于LLM的智能体通过标准化接口安全、一致地与外部服务和数据源交互。

A2A（智能体间协作）：来自不同供应商和模型架构的智能体可以发现同伴、委托责任，并 作为平等参与者协作管理复杂任务。



### 3 Deep Research: Search Engine, Tool Use,Workflow, Tuning, Non-parametric Continual Learning

与传统的RAG-based方法比较：DR代理通过将动态检索、实时工具使用和自适应推理集成到一个统一系统中，并在实时中管理多阶段研究任务，提供了更高的自主性、上下文感知和准确性。

#### 3.1 Search Engine: API vs. Browser

1) 基于**API**的搜索引擎：它们与结构化数据源交互（如科学数据库API），实现高效检索组织化信息，是一种快速、高效且可扩展的方式。
   但可能难以处理深层嵌套、客户端JavaScript渲染的内容、交互式组件或身份验证障碍。
2) 基于**浏览器**的搜索引擎：通过模拟与网页的类人交互，促进动态或非结构化内容的实时提取，提升外部知识全面性。但存在更大的延迟、资源消耗和处理页面变化和错误时的复杂性。

#### 3.2 Tool Use: Empowering Agents with Extended Functionalities

**Code Interpreter**.代码解释器功能使深度研究代理能够在推理过程中执行脚本，允许它们进行数据处理、算法 验证和模型模拟。

**Data Analytics**.通过集成数据分析模块，深度研究代理将原始检索转化为结构化洞察，通过计算汇总统计、生 成交互式可视化以及进行定量模型评估，从而加速假设检验和决策。

**Multimodal Processing and Generation**.DR代理能够在统一的推理管道中集成、分析和生成文本、图像、 音频和视频等异构数据，从而丰富其上下文理解并拓宽其输出范围。

**Deep Research Agent with Computer Use**.DR代理通过集成计算机辅助任务执行能力（即计算机使用） 逐步扩展，实现从“思考”到“行动”的全流程自动化。

#### 3.3 Architecture andWorkflow

静态工作流： 静态工作流依赖于手动预定义的任务管道，将研究过程分解为由专用代理执行的顺序子任务。

**动态工作流** ：为了克服静态工作流在灵活性和泛化能力方面的局限性，动态工作流支持自适应任务规划， 允许代理根据迭代反馈和不断变化的上下文动态重新配置任务结构。

1. 规划策略：三种类型包括：仅规划（直接规划而不澄清用户意图）、意图到规划（澄清意图后再规划）和统一意图规划（生成计划并请求用户确认）。
2. 动态**单代理**系统： 将规划、工具调用和执行统一在单一的LRM（Language Reasoning Model）内，将任务管理简化为一个连贯 的认知循环。其能够在整个工作流程中进行端到端的强化学习（RL）优化，促进推理、 规划和工具调用的**更平滑、更连贯的集成**。
3. 动态**多代理**系统：动态多代理系统利用多个专业代理协同执行通过自适应规划策略生成和动态分配的子任务。其可以有效处理复杂、可并行化的研究任务，从而增强开放式研究场景中的**灵活性和可扩展性**。
4. **记忆机制**使 DR 代理能够持续捕获、组织和回忆跨多轮检索的相关信息，从而减少冗余查询并提高 DR 任务的效率和连贯性。可分为三大优化策略：(i) 扩展 上下文窗口长度； (ii) 压缩中间步骤； (iii) 利用外部结构化存储临时结果。

#### 3.4 Tuning: Beyond Prompting toward Capability Enhancement

##### Parametric Approaches.

**SFT-based Optimization**：基于提示的方法虽然对于快速适应很有效，但根本上受限于骨干大型语言模型的内在泛化能力，并且在复杂的任务设置中通常表现出有限的鲁棒性。为此，现在旨在系统地优化DR代理**关键组件**的微调方法、减少冗余的外部查询 或是 减少对数据集依赖（DR更多要依靠于检索）。

**Reinforcement Learning-based Optimisation**：基于RL的方法通过直接增强DR代理的**自适应能力**和在多样化任务中的**泛化能力**来优化DR代理，提升检索的相关性。
用近端策略优化（PPO） <   组相对策略优化（GRPO）

##### Non-parametric Continual Learning.

DR 代理严重依赖 LRMs，并经常使用复杂的分层工作流，所以非参数的持续学习可以使DR代理通过在**与外部环境持续交互**中优化外部内存、工作流和工具配置来在运行时改进其能力，**而不是通过更新内部权重**。这种非参数持续学习范式实现了高效的在线适应，数据计算开销最小，非常适合具有复杂架构的 DR 代理。尤其是基于案例的推理（CBR），其在轨迹级别运行，并强调**以推理为中心**的内存组织。



### 4 Industrial Applications of Deep Research Agents

对几个案例分析、说明特点。



### 5 Benchmarks for DR Agent

问答 (QA) 基准测试范围从单轮事实性查询到复杂研 究式问题，评估代理的事实性知识、领域特定推理以及定位和整合相关信息的能力。人类最后考试（**HLE**）
任务执行基准测试：通过衡量代理执行端到端研究任务的效果，评估更广泛的能力，如长时规划、多模态理解、工具使用和环境 交互。**GAIA**



### 6 Challenge and Future Directions

Broaden Information Source：为满足复杂任务的信息需求，当前的DR代理采用静态知识库（如RAG方法，搜索内容不够）或完全依赖搜索引擎和浏览器（只能识别公开网页，又有反爬虫机制等），交互效果差。
设计AI原生浏览器

Fact Checking：结构化验证循环和自我反思能力，具体来说，一旦代理生成了一个初步答案，它会主动发起交叉核查，寻找独立来源 以确认同一事实，并搜索矛盾的证据。

Asynchronous Parallel Execution：异步并行执行，实现更鲁棒和高效端到 端研究工作流。

Tool-Integrated Reasoning：一种超越了简单的工具使用，包含了复杂的、多步骤的推理和动态工具集成的范式，要求逻辑顺序调用适当的工具，并且根据中间结果自适应地调整其推理路径。

Benchmark Misalignment.：traditional QA在网上可以找到答案致使DR评估不准确。需要开发综合基准测试

Parametric Optimisation of Multi-Agent Architectures：将工作分发给多个DR代理，实现多个代理间的高效协调。
两个有前景的未来方向：(i) 采用分层强化学习 (HRL)，该技术引入了**分层的内部奖励机制**，以促进高效的反馈传播并促进代理间的协同学习；或实施一个包含多个精炼 阶段的**后训练优化**管道，这些阶段专门针对DR任务设计，可以迭代增强代理间交互，从而提高整体系统稳 定性和适应性；(ii) 采用一个**基于RL的专用调度代理**，该代理旨在根据实时性能指标动态分配子任务和 调整执行顺序。

Self-Evolving Language Model Agents.：(i) 综合基于案例的推理框架。(ii)自主工作流演化 ，应用进化算法或自适应图优化来动态探索、修改和细化执行计划，可以提高效率和灵活性。



### 7 Conclusion

地回顾了DR代理的最新进展、将现有方法论分为基于提示的、基于微调的和基于强化学习的方法、还考察了行业领导者开发的突出DR代理系统。

Limitation：括跨多样化任务的泛化能力有限、不灵活的任务工作流、集成 粒度外部工具的困难，以及与高级规划和优化相关的巨大计算复杂性。