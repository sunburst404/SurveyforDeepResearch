# SurveyforDeepResearch
### å‡†å¤‡é˜…è¯»çš„è®ºæ–‡

##### âœ…A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications

[A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications](https://arxiv.org/pdf/2506.12594v1)

> This survey examines the rapidly evolving field of Deep Research systems -- AI-powered applications that automate complex research workflows through the integration of large language models, advanced information retrieval, and autonomous reasoning capabilities. We analyze more than 80 commercial and non-commercial implementations that have emerged since 2023, including OpenAI/Deep Research, Gemini/Deep Research, Perplexity/Deep Research, and numerous open-source alternatives. Through comprehensive examination, we propose a novel hierarchical taxonomy that categorizes systems according to four fundamental technical dimensions: foundation models and reasoning engines, tool utilization and environmental interaction, task planning and execution control, and knowledge synthesis and output generation. We explore the architectural patterns, implementation approaches, and domain-specific adaptations that characterize these systems across academic, scientific, business, and educational applications. Our analysis reveals both the significant capabilities of current implementations and the technical and ethical challenges they present regarding information accuracy, privacy, intellectual property, and accessibility. The survey concludes by identifying promising research directions in advanced reasoning architectures, multimodal integration, domain specialization, human-AI collaboration, and ecosystem standardization that will likely shape the future evolution of this transformative technology. By providing a comprehensive framework for understanding Deep Research systems, this survey contributes to both the theoretical understanding of AI-augmented knowledge work and the practical development of more capable, responsible, and accessible research technologies. The paper resources can be viewed at [this https URL](https://github.com/scienceaix/deepresearch).

ğŸ“¢è¿™ç¯‡surveyçš„ä»‹ç»deep researchçš„æ¦‚å¿µï¼ŒåŒºåˆ«äºLLmçš„æ–¹é¢æˆ‘è§‰å¾—è¿˜æ˜¯å¾ˆæ¸…æ™°çš„ï¼Œdeep researchçš„ä¸ºä»€ä¹ˆå…·æœ‰å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ä¹Ÿæ˜¯è§£é‡Šå¾ˆæ¸…æ™°ï¼Œå¼ºè°ƒå…¶å…·å¤‡**è‡ªä¸»å·¥ä½œæµèƒ½åŠ›**ã€**ä¸“ç”¨ç ”ç©¶å·¥å…·é›†æˆ**å’Œ**ç«¯åˆ°ç«¯ç ”ç©¶ç¼–æ’**ä¸‰å¤§æ ¸å¿ƒç‰¹å¾ã€‚å…¶è°ƒæŸ¥äº†å¾ˆå¤šçš„deep researchå¹³å°ï¼ŒåŒ…æ‹¬äº†OpenAIã€Geminiã€Perplexityã€dzhngå’Œä¸€äº›å¼€æºçš„/deep-researchï¼Œå¹¶ä¸”æå‡ºæ¯ä¸€ä¸ªdeep researchè¯¥æœ‰çš„ç‰¹ç‚¹æ—¶éƒ½ä¼šç»“åˆå…·ä½“ä¾‹å­é˜è¿°ã€‚
ä½†æ˜¯æ¡†æ¶æˆ‘è§‰å¾—ä¸æ˜¯å¾ˆæ¸…æ¥šï¼Œä¸‰ä¸ªæ¡†æ¶çš„å›¾åªæ˜¯åŠŸèƒ½çš„å †ç Œï¼›deep researchçš„æµç¨‹ä¹Ÿæœªæ¶‰åŠï¼Œè¯„ä¼°æˆ‘è§‰å¾—ä¹Ÿæœ‰ç‚¹ä¸»è§‚äº†ã€‚



##### âœ…Deep Research System Card 

https://cdn.openai.com/deep-research-system-card.pdf

ğŸ“¢æœ¬èº«çš„æŠ€æœ¯å®ç°æ˜¯ä¸€ç‚¹ä¸ç»™çœ‹å•Šï¼Œå…‰æ˜¯è¯„ä¼°è‡ªå·±çš„deep researché£é™©è¿˜è¡Œå’Œæ•°æ®é›†æœ‰æ›´æ–°ï¼ˆæ›´æ–°äº†å•¥ä¹Ÿä¸æ™“å¾—ï¼‰ï¼Œæ²¡å•¥å±å®³æ€§æ‰€ä»¥å¯ä»¥ç»™proç”¨æˆ·å•†ç”¨ä½¿ç”¨ã€‚**è¯„ä¼°æ–¹æ³•**å¯ä»¥å€Ÿé‰´ã€‚



##### âœ…A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges

[[2508.05668v1\] A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges](https://arxiv.org/abs/2508.05668v1)\

ğŸ“¢ğŸ‘‹è¿™ç¯‡surveyä¸»è¦èšç„¦çš„æ˜¯ Search Agentsæœç´¢ä¿¡æ¯çš„æ–¹æ³•ã€ä¼˜åŒ–agentsæ¶æ„æ–¹æ³•ã€æ·±åº¦ä¿¡æ¯æŒ–æ˜æ–¹é¢çš„èƒ½åŠ›ï¼Œè€ŒDeep Research ä»£è¡¨äº†ä¸€ç§æ›´å…ˆè¿›çš„ä»£ç†å¼å®ç°ï¼Œå…·å¤‡å¤šæ­¥æ¨ç†ã€ä»£ç æ‰§è¡Œå’ŒåŠ¨æ€è·¯å¾„è°ƒæ•´ç­‰èƒ½åŠ›ã€‚æ²¡æ€ä¹ˆè¯»



##### Toolformer: Language Models Can Teach Themselves to Use Tools

[2302.04761](https://arxiv.org/pdf/2302.04761)è®­ç»ƒLLMsä½¿ç”¨å·¥å…·api



##### âœ…DEEP RESEARCH AGENTS: A SYSTEMATIC EXAMINATION AND ROADMAP

https://arxiv.org/pdf/2506.18096

ğŸ’¥ğŸ’¥ğŸ’¥å¯¹æ„æˆæ·±åº¦ç ”ç©¶ä»£ç†çš„åŸºç¡€æŠ€æœ¯å’Œæ¶æ„ç»„ä»¶è¿›è¡Œäº†è¯¦ç»†åˆ†æ





##### Deep Research: A Survey of Autonomous Research Agents

[Deep Research: A Survey of Autonomous Research Agents](https://arxiv.org/pdf/2508.12752)

Abstract: The rapid advancement of large language models (LLMs) has driven the development of agentic systems capable of autonomously performing complex tasks. Despite their impressive capabilities, LLMs remain constrained by their internal knowledge boundaries. To overcome these limitations, the paradigm of deep research has been proposed, wherein agents actively engage in planning, retrieval, and synthesis to generate comprehensive and faithful analytical reports grounded in web-based evidence. In this survey, we provide a systematic overview of the deep research pipeline, which comprises four core stages: planning, question developing, web exploration, and report generation. For each stage, we analyze the key technical challenges and categorize representative methods developed to address them. Furthermore, we summarize recent advances in optimization techniques and benchmarks tailored for deep research. Finally, we discuss open challenges and promising research directions, aiming to chart a roadmap toward building more capable and trustworthy deep research agents. 



##### Universal Deep Research: Bring Your Own Model and Strategy

[Universal Deep Research: Bring Your Own Model and Strategy](https://arxiv.org/pdf/2509.00244)https://arxiv.org/search/?searchtype=author&query=Molchanov%2C+P)   è‡ªå®šä¹‰deep researchçš„ç ”ç©¶ç­–ç•¥

Abstract: Deep research tools are among the most impactful and most commonly encountered agentic systems today. We observe, however, that each deep research agent introduced so far is hard-coded to carry out a particular research strategy using a fixed choice of tools. We introduce Universal Deep Research (UDR), a generalist agentic system that wraps around any language model and enables the user to create, edit, and refine their own entirely custom deep research strategies without any need for additional training or finetuning. To showcase the generality of our system, we equip UDR with example minimal, expansive, and intensive research strategies, and provide a user interface to facilitate experimentation with the system.



##### Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training

[Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training](https://arxiv.org/pdf/2508.00414)    è…¾è®¯ai labçš„ä¸€ä¸ªå¼€æºçš„deep research agent



##### DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments

[[2504.03160\] DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments](https://arxiv.org/abs/2504.03160)   ä¸Šäº¤å’Œå…¶ä»–å®éªŒå®¤ å¼€æºçš„

> Large Language Models (LLMs) equipped with web search capabilities have demonstrated impressive potential for deep research tasks. However, current approaches predominantly rely on either manually engineered prompts (prompt engineering-based) with brittle performance or reinforcement learning within controlled Retrieval-Augmented Generation (RAG) environments (RAG-based) that fail to capture the complexities of real-world interaction. In this paper, we introduce DeepResearcher, the first comprehensive framework for end-to-end training of LLM-based deep research agents through scaling reinforcement learning (RL) in real-world environments with authentic web search interactions. Unlike RAG-based approaches that assume all necessary information exists within a fixed corpus, our method trains agents to navigate the noisy, unstructured, and dynamic nature of the open web. We implement a specialized multi-agent architecture where browsing agents extract relevant information from various webpage structures and overcoming significant technical challenges. Extensive experiments on open-domain research tasks demonstrate that DeepResearcher achieves substantial improvements of up to 28.9 points over prompt engineering-based baselines and up to 7.2 points over RAG-based RL agents. Our qualitative analysis reveals emergent cognitive behaviors from end-to-end RL training, including the ability to formulate plans, cross-validate information from multiple sources, engage in self-reflection to redirect research, and maintain honesty when unable to find definitive answers. Our results highlight that end-to-end training in real-world web environments is not merely an implementation detail but a fundamental requirement for developing robust research capabilities aligned with real-world applications. We release DeepResearcher at [this https URL](https://github.com/GAIR-NLP/DeepResearcher).



##### AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents

[[2502.05957\] AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents](https://arxiv.org/abs/2502.05957)  æ¸¯å¤§çš„å¼€æºé¡¹ç›®ï¼Œåº”è¯¥æ˜¯é›†æˆäº†ä¹‹å‰çš„deep researché¡¹ç›®

Large Language Model (LLM) Agents have demonstrated remarkable capabilities in task automation and intelligent decision-making, driving the widespread adoption of agent development frameworks such as LangChain and AutoGen. However, these frameworks predominantly serve developers with extensive technical expertise - a significant limitation considering that only 0.03 % of the global population possesses the necessary programming skills. This stark accessibility gap raises a fundamental question: Can we enable everyone, regardless of technical background, to build their own LLM agents using natural language alone? To address this challenge, we introduce AutoAgent-a Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone. Operating as an autonomous Agent Operating System, AutoAgent comprises four key components: i) Agentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing File System, and iv) Self-Play Agent Customization module. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention. Beyond its code-free agent development capabilities, AutoAgent also serves as a versatile multi-agent system for General AI Assistants. Comprehensive evaluations on the GAIA benchmark demonstrate AutoAgent's effectiveness in generalist multi-agent tasks, surpassing existing state-of-the-art methods. Furthermore, AutoAgent's Retrieval-Augmented Generation (RAG)-related capabilities have shown consistently superior performance compared to many alternative LLM-based solutions.



##### OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation

[[2505.23885\] OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation](https://arxiv.org/abs/2505.23885)æ¸¯å¤§å’Œcamel-ai åšçš„å¼€æº

Large Language Model (LLM)-based multi-agent systems show promise for automating real-world tasks but struggle to transfer across domains due to their domain-specific nature. Current approaches face two critical shortcomings: they require complete architectural redesign and full retraining of all components when applied to new domains. We introduce Workforce, a hierarchical multi-agent framework that decouples strategic planning from specialized execution through a modular architecture comprising: (i) a domain-agnostic Planner for task decomposition, (ii) a Coordinator for subtask management, and (iii) specialized Workers with domain-specific tool-calling capabilities. This decoupling enables cross-domain transferability during both inference and training phases: During inference, Workforce seamlessly adapts to new domains by adding or modifying worker agents; For training, we introduce Optimized Workforce Learning (OWL), which improves generalization across domains by optimizing a domain-agnostic planner with reinforcement learning from real-world feedback. To validate our approach, we evaluate Workforce on the GAIA benchmark, covering various realistic, multi-domain agentic tasks. Experimental results demonstrate Workforce achieves open-source state-of-the-art performance (69.70%), outperforming commercial systems like OpenAI's Deep Research by 2.34%. More notably, our OWL-trained 32B model achieves 52.73% accuracy (+16.37%) and demonstrates performance comparable to GPT-4o on challenging tasks. To summarize, by enabling scalable generalization and modular domain transfer, our work establishes a foundation for the next generation of general-purpose AI assistants.



å…¶ä»–å¼€æºdeep researché¡¹ç›®  Aworldï¼Œopenmanus



##### Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models

[[2411.19443\] Auto-RAG: Autonomous Retrieval-Augmented Generation for Large Language Models](https://arxiv.org/abs/2411.19443)

> Iterative retrieval refers to the process in which the model continuously queries the retriever during generation to enhance the relevance of the retrieved knowledge, thereby improving the performance of Retrieval-Augmented Generation (RAG). Existing work typically employs few-shot prompting or manually constructed rules to implement iterative retrieval. This introduces additional inference overhead and overlooks the remarkable reasoning capabilities of Large Language Models (LLMs). In this paper, we introduce Auto-RAG, an autonomous iterative retrieval model centered on the LLM's powerful decision-making capabilities. Auto-RAG engages in multi-turn dialogues with the retriever, systematically planning retrievals and refining queries to acquire valuable knowledge. This process continues until sufficient external information is gathered, at which point the results are presented to the user. To this end, we develop a method for autonomously synthesizing reasoning-based decision-making instructions in iterative retrieval and fine-tuned the latest open-source LLMs. The experimental results indicate that Auto-RAG is capable of autonomous iterative interaction with the retriever, effectively leveraging the remarkable reasoning and decision-making abilities of LLMs, which lead to outstanding performance across six benchmarks. Further analysis reveals that Auto-RAG can autonomously adjust the number of iterations based on the difficulty of the questions and the utility of the retrieved knowledge, without requiring any human intervention. Moreover, Auto-RAG expresses the iterative retrieval process in natural language, enhancing interpretability while providing users with a more intuitive experience\footnote{Code is available at \url{[this https URL](https://github.com/ictnlp/Auto-RAG)}.



##### Open Data Synthesis For Deep Research

[Open Data Synthesis For Deep Research](https://arxiv.org/pdf/2509.00375)            å¼•å…¥äº†InfoSeekï¼Œè¿™æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„æ¡†æ¶ï¼Œç”¨äºç»¼åˆå¤æ‚çš„æ·±åº¦ç ”ç©¶ä»»åŠ¡ã€‚å°†é—®é¢˜åˆ†è§£æˆå­é—®é¢˜ã€åè°ƒå¤šæ­¥æ¨ç†ã€‚



##### Deep Research Bench: Evaluating AI Web Research Agents

[2506.06287](https://arxiv.org/pdf/2506.06287)è¯„ä¼°æ–‡ç« 



##### Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents

[Deep Research Comparator: A Platform For Fine-grained Human Annotations of Deep Research Agents](https://arxiv.org/pdf/2507.05495)               æ·±åº¦ç ”ç©¶æ¯”è¾ƒå™¨



### æ¯æ—¥è¿›åº¦

| æ—¶é—´     |                                                              |
| -------- | ------------------------------------------------------------ |
| 2025.9.1 | é˜…è¯»A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applicationsï¼Œå’Œä¹‹å‰è¯»çš„ä¸€èµ·åŠ èµ·æ¥è¯»åˆ°äº†ç¬¬å››ç« ã€‚papersæ–‡ä»¶å¤¹é‡Œæœ‰è®ºæ–‡åŸæ–‡ä¸æ‰¹æ³¨ã€è®ºæ–‡æ¦‚å†µã€‚ |
| 9.2      | çœ‹åˆ°ç¬¬ä¸ƒç«                                                    |
| 9.3-9.4  | çœ‹å®Œä½™ä¸‹ç« èŠ‚ï¼Œæ€»ç»“äº†ä¸€ä¸‹å…¨ç¯‡ï¼ˆä¸»è¦æ˜¯å‰å‡ ç« ï¼‰                 |
| 9.5-9.6  | çœ‹å®Œopenaiçš„Deep Research System Card ï¼Œäº†è§£äº†ä¸€äº›è¯„ä¼°ä¸å…¶èƒ½åŠ›ã€å­˜åœ¨é£é™© |
| 9.7-9.8  | åˆæ‰¾äº†ä¸€äº›è®ºæ–‡å’Œå¼€æºçš„é¡¹ç›®å‡†å¤‡çœ‹ï¼Œçœ‹éƒ¨åˆ†äº†DEEP RESEARCH AGENTS:A SYSTEMATIC EXAMINATION AND ROADMAPï¼Œæ¯”ç¬¬ä¸€ç¯‡surveyæ¸…æ™°ä¸€äº›ï¼Œæ¡†æ¶æ›´ä¸ºæ¸…æ¥šä½†å…·ä½“ç»†èŠ‚æœªäº¤ä»£éœ€è¦æ‰¾å¼€æºçš„çœ‹ã€‚ |
| 9.9      | è¯»å®Œäº†DEEP RESEARCH AGENTS:A SYSTEMATIC EXAMINATION AND ROADMAP |







### æœ‰å›°éš¾çš„åœ°æ–¹

| æ—¶é—´ | ä½ç½®                        | å…·ä½“é—®é¢˜                                                     | æ˜¯å¦è§£å†³ |
| ---- | --------------------------- | ------------------------------------------------------------ | -------- |
| 9.1  | å…³äºdeep researchçš„å®é™…ç”¨ä¾‹ | æˆ‘è¿˜æ²¡æœ‰ç”¨è¿‡ï¼Œå¾—èŠ±æ—¶é—´è¯•ä¸€ä¸‹æ¯”å¦‚perplexityï¼Œopenai deep research | no       |
| 9.6  | deep researchæ¡†æ¶           | è¿˜è¦æ‰¾ä¸ªå¼€æºçš„çœ‹çœ‹æ¡†æ¶ï¼Œæµç¨‹ä¹‹ç±»çš„                           | no       |
|      |                             |                                                              |          |
|      |                             |                                                              |          |
